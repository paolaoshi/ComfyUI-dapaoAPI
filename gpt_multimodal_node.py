"""
å¤§ç‚® API - GPT å¤šæ¨¡æ€å¯¹è¯èŠ‚ç‚¹
æä¾› GPT ç³»åˆ—æ¨¡å‹ï¼ˆå¦‚ GPT-4o, o1 ç­‰ï¼‰çš„å¤šæ¨¡æ€å¯¹è¯åŠŸèƒ½

ä½œè€…ï¼š@ç‚®è€å¸ˆçš„å°è¯¾å ‚
ç‰ˆæœ¬ï¼šv1.0.1
"""

import os
import json
import random
import requests
import base64
import io
from PIL import Image
import numpy as np
import torch
from io import BytesIO

# å°è¯•å¯¼å…¥ urllib3
try:
    import urllib3
    # ç¦ç”¨ SSL è­¦å‘Š
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except ImportError:
    pass

# è·å–å½“å‰ç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
GPT_CONFIG_FILE = os.path.join(CURRENT_DIR, 'gpt_config.json')

print(f"[dapaoAPI] GPT å¤šæ¨¡æ€èŠ‚ç‚¹æ¨¡å—å·²åŠ è½½")

# ==================== è¾…åŠ©å‡½æ•° ====================

def _log_info(message):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°"""
    print(f"[dapaoAPI-GPT] ä¿¡æ¯ï¼š{message}")


def _log_warning(message):
    """ç»Ÿä¸€çš„è­¦å‘Šè¾“å‡ºå‡½æ•°"""
    print(f"[dapaoAPI-GPT] è­¦å‘Šï¼š{message}")


def _log_error(message):
    """ç»Ÿä¸€çš„é”™è¯¯è¾“å‡ºå‡½æ•°"""
    print(f"[dapaoAPI-GPT] é”™è¯¯ï¼š{message}")


def encode_image_tensor(image_tensor) -> str:
    """å°†ComfyUI tensorè½¬æ¢ä¸ºbase64 PNG"""
    # Convert tensor to numpy array
    if hasattr(image_tensor, 'cpu'):
        image_np = image_tensor.cpu().numpy()
    else:
        image_np = np.array(image_tensor)
    
    # Convert to 0-255 range
    if image_np.max() <= 1.0:
        image_np = (image_np * 255).astype(np.uint8)
    
    # Handle batch dimension if present (take first image)
    if len(image_np.shape) == 4:
        image_np = image_np[0]
        
    # Create PIL Image
    img = Image.fromarray(image_np)
    
    # Encode to PNG
    buffer = BytesIO()
    img.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


def get_gpt_config():
    """è¯»å– GPT é…ç½®æ–‡ä»¶"""
    default_config = {
        "gpt_api_key": "",
        "gpt_base_url": "https://ai.t8star.cn/v1",
        "gpt_model": "gpt-5.1-thinking",
        "timeout": 120
    }
    
    try:
        if os.path.exists(GPT_CONFIG_FILE):
            with open(GPT_CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return {**default_config, **config}
        else:
            return default_config
    except Exception as e:
        _log_error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return default_config


# ==================== èŠ‚ç‚¹ç±» ====================

class GPT_Multimodal_Chat:
    """
    GPT å¤šæ¨¡æ€å¯¹è¯èŠ‚ç‚¹
    
    æ”¯æŒ GPT-4o, o1 ç­‰æ¨¡å‹çš„å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰
    
    ä½œè€…ï¼š@ç‚®è€å¸ˆçš„å°è¯¾å ‚
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        config = get_gpt_config()
        # å¸¸è§ GPT æ¨¡å‹åˆ—è¡¨
        model_list = [
            "gpt-5.1-thinking",
            "gpt-5.1-thinking-all",
            "gpt-5.1",
            "gpt-5.1-all"
        ]
        
        return {
            "required": {
                "ğŸ¯ ç³»ç»Ÿè§’è‰²": ("STRING", {
                    "multiline": True,
                    "default": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚",
                    "placeholder": "å®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼..."
                }),
                
                "ğŸ’¬ ç”¨æˆ·è¾“å…¥": ("STRING", {
                    "multiline": True,
                    "default": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚",
                    "placeholder": "è¾“å…¥ä½ æƒ³è¦å‘é€çš„æ¶ˆæ¯..."
                }),
                
                "ğŸ¤– æ¨¡å‹é€‰æ‹©": (model_list, {
                    "default": config.get("gpt_model", "gpt-5.1-thinking")
                }),
                
                "ğŸ”‘ APIå¯†é’¥": ("STRING", {
                    "default": "",
                    "placeholder": "ç•™ç©ºåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–"
                }),
                
                "ğŸ“Š è¾“å‡ºè¯­è¨€": (["ä¸­æ–‡", "è‹±æ–‡"], {
                    "default": "ä¸­æ–‡"
                }),
                
                "ğŸŒ¡ï¸ æ¸©åº¦": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ (å¯¹äºo1/æ¨ç†æ¨¡å‹å¯èƒ½æ— æ•ˆ)"
                }),
                
                "ğŸ² top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p æ ¸é‡‡æ ·å‚æ•°"
                }),
                
                "ğŸ“ æœ€å¤§ä»¤ç‰Œ": ("INT", {
                    "default": 4096,
                    "min": 256,
                    "max": 128000,
                    "step": 256,
                    "tooltip": "ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§tokenæ•°é‡"
                }),
                
                "ğŸ² éšæœºç§å­": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 9223372036854775807,
                    "tooltip": "éšæœºç§å­å€¼ï¼ˆ0è¡¨ç¤ºä¸ä½¿ç”¨å›ºå®šç§å­ï¼‰"
                }),
                
                "ğŸ›ï¸ ç§å­æ§åˆ¶": (["å›ºå®š", "éšæœº", "é€’å¢"], {
                    "default": "éšæœº",
                    "tooltip": "å›ºå®š: ä½¿ç”¨ä¸Šæ–¹ç§å­å€¼; éšæœº: æ¯æ¬¡ç”Ÿæˆæ–°ç§å­; é€’å¢: ç§å­å€¼+1"
                }),
            },
            "optional": {
                "ğŸ–¼ï¸ å›¾åƒ1": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ2": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ3": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ4": ("IMAGE",),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("ğŸ’­ AIå›å¤", "ğŸ“„ å®Œæ•´å“åº”", "â„¹ï¸ å¤„ç†ä¿¡æ¯")
    FUNCTION = "chat"
    CATEGORY = "ğŸ¤–dapaoAPI/GPT"
    DESCRIPTION = "GPT å¤šæ¨¡æ€å¯¹è¯ (OpenAI/T8) | ä½œè€…: @ç‚®è€å¸ˆçš„å°è¯¾å ‚"
    OUTPUT_NODE = False
    
    def __init__(self):
        self.config = get_gpt_config()
        self.last_seed = -1

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        seed_control = kwargs.get("ğŸ›ï¸ ç§å­æ§åˆ¶", "éšæœº")
        seed = kwargs.get("ğŸ² éšæœºç§å­", -1)
        
        # éšæœºå’Œé€’å¢æ¨¡å¼ä¸‹ï¼Œå¼ºåˆ¶æ›´æ–° (è¿”å› NaN)
        if seed_control in ["éšæœº", "é€’å¢"]:
            return float("nan")
        
        # å›ºå®šæ¨¡å¼ä¸‹ï¼Œä»…å½“ç§å­å€¼å˜åŒ–æ—¶æ›´æ–°
        return seed
    
    def chat(self, **kwargs):
        """ä¸»å‡½æ•°ï¼šGPTå¯¹è¯"""
        
        # === å‚æ•°è§£æ ===
        user_message = kwargs.get("ğŸ’¬ ç”¨æˆ·è¾“å…¥", "")
        system_prompt = kwargs.get("ğŸ¯ ç³»ç»Ÿè§’è‰²", "")
        api_key = kwargs.get("ğŸ”‘ APIå¯†é’¥", "")
        model_name = kwargs.get("ğŸ¤– æ¨¡å‹é€‰æ‹©", "gpt-5.1-thinking")
        output_lang = kwargs.get("ğŸ“Š è¾“å‡ºè¯­è¨€", "ä¸­æ–‡")
        temperature = kwargs.get("ğŸŒ¡ï¸ æ¸©åº¦", 0.7)
        top_p = kwargs.get("ğŸ² top_p", 0.9)
        max_tokens = kwargs.get("ğŸ“ æœ€å¤§ä»¤ç‰Œ", 4096)
        seed = kwargs.get("ğŸ² éšæœºç§å­", 0)
        seed_control = kwargs.get("ğŸ›ï¸ ç§å­æ§åˆ¶", "éšæœº")
        
        # å›¾åƒè¾“å…¥
        image1 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ1")
        image2 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ2")
        image3 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ3")
        image4 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ4")
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒ
        images = [img for img in [image1, image2, image3, image4] if img is not None]
        
        # === çŠ¶æ€ä¿¡æ¯ ===
        status_info = []
        
        # === æ£€æŸ¥æ¶ˆæ¯ ===
        if not user_message.strip():
            error_msg = "âŒ é”™è¯¯ï¼šè¯·è¾“å…¥ç”¨æˆ·æ¶ˆæ¯"
            _log_error(error_msg)
            return ("", "", error_msg)
        
        # === è·å– API å¯†é’¥ ===
        if not api_key:
            api_key = self.config.get("gpt_api_key", "")
        
        if not api_key:
            error_msg = "âŒ é”™è¯¯ï¼šè¯·é…ç½® GPT API Key\n\nè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š\n1. åœ¨èŠ‚ç‚¹å‚æ•°ä¸­è¾“å…¥ API å¯†é’¥\n2. ç¼–è¾‘ gpt_config.json æ–‡ä»¶é…ç½®"
            _log_error(error_msg)
            return ("", "", error_msg)
        
        # === ç§å­å¤„ç† ===
        # ç¡®ä¿ç§å­åœ¨ signed 64-bit æ•´æ•°èŒƒå›´å†… (APIé™åˆ¶)
        MAX_SEED = 9223372036854775807
        
        if seed_control == "å›ºå®š":
            effective_seed = seed
            seed_mode = "å›ºå®š"
        elif seed_control == "éšæœº":
            effective_seed = random.randint(0, MAX_SEED)
            seed_mode = "éšæœº"
        elif seed_control == "é€’å¢":
            if self.last_seed == -1:
                effective_seed = seed if seed != -1 else random.randint(0, MAX_SEED)
            else:
                effective_seed = self.last_seed + 1
            seed_mode = "é€’å¢"
        else:
            effective_seed = random.randint(0, MAX_SEED)
            seed_mode = "éšæœº"
        
        # ç¡®ä¿æœ€ç»ˆç§å­åœ¨æœ‰æ•ˆèŒƒå›´å†…
        effective_seed = effective_seed % MAX_SEED
        
        self.last_seed = effective_seed
        random.seed(effective_seed)
            
        status_info.append(f"ğŸ¤– æ¨¡å‹ï¼š{model_name}")
        status_info.append(f"ğŸ² ç§å­ï¼š{effective_seed} (æ¨¡å¼: {seed_mode})")
        if images:
            status_info.append(f"ğŸ–¼ï¸ å›¾åƒè¾“å…¥ï¼š{len(images)} å¼ ")
        _log_info(f"ä½¿ç”¨ç§å­ï¼š{effective_seed}ï¼Œæ¨¡å¼ï¼š{seed_mode}")
            
        try:
            # === è°ƒç”¨ API ===
            _log_info("æ­£åœ¨è°ƒç”¨ GPT API è¿›è¡Œå¯¹è¯...")
            
            base_url = self.config.get("gpt_base_url", "https://ai.t8star.cn/v1")
            url = f"{base_url}/chat/completions"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            
            messages = []
            
            # å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œè¯­è¨€è®¾ç½®
            final_system_prompt = system_prompt
            if output_lang == "ä¸­æ–‡":
                lang_instruction = "è¯·ç”¨ä¸­æ–‡è¯¦ç»†å›ç­”ï¼Œæä¾›å°½å¯èƒ½å®Œæ•´å’Œè¯¦ç»†çš„æè¿°ã€‚"
            else:
                lang_instruction = "Please answer in English with detailed and comprehensive description."
            
            if final_system_prompt.strip():
                final_system_prompt = f"{final_system_prompt}\n\n{lang_instruction}"
            else:
                final_system_prompt = lang_instruction
            
            # OpenAI o1 ç³»åˆ—æ¨¡å‹ä¸æ”¯æŒ system roleï¼Œéœ€è¦è½¬ä¸º user role æˆ–è€… developer role
            # ä½†å¤§éƒ¨åˆ† T8/OpenAI å…¼å®¹æ¥å£ç›®å‰å¯¹ o1 çš„æ”¯æŒå„å¼‚ï¼Œé€šå¸¸å»ºè®®æŠŠ system prompt åˆå¹¶åˆ° user prompt
            # æˆ–è€… T8 å·²ç»åšäº†é€‚é…ã€‚ä¸ºäº†å®‰å…¨èµ·è§ï¼Œå¦‚æœæ˜¯ o1 æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸ªç®€å•åˆ¤æ–­
            is_reasoning_model = "o1" in model_name.lower() or "reasoning" in model_name.lower()
            
            if is_reasoning_model:
                # å¯¹äº o1 æ¨¡å‹ï¼Œæœ‰äº›æ¥å£ä¸æ”¯æŒ system roleï¼Œæš‚æ—¶å…ˆä¿ç•™ï¼Œå¦‚æœæŠ¥é”™å†æ”¹
                # æˆ–è€…ç›´æ¥å°† system prompt ä½œä¸ºç¬¬ä¸€æ¡ user æ¶ˆæ¯
                # è¿™é‡Œçš„å¤„ç†æ–¹å¼ï¼šä»ç„¶ä¿ç•™ systemï¼Œä½†å¦‚æœæŠ¥é”™ 400 (unsupported role)ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦åé¦ˆ
                # ä¸è¿‡ T8 æ—¢ç„¶å…¼å®¹ï¼Œå¯èƒ½å·²ç»å¤„ç†äº†ã€‚
                # æŒ‰ç…§ OpenAI å®˜æ–¹ o1-preview æ–‡æ¡£ï¼Œsystem message æ˜¯æ”¯æŒçš„ï¼Œä½†æ˜¯ä¸å»ºè®®ç”¨å¤æ‚çš„ system instruction
                # è¿˜æ˜¯ç…§å¸¸å‘é€ system message
                messages.append({"role": "system", "content": final_system_prompt})
            else:
                messages.append({"role": "system", "content": final_system_prompt})
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_content = []
            
            # 1. æ·»åŠ æ–‡æœ¬
            if user_message.strip():
                user_content.append({"type": "text", "text": user_message})
            
            # 2. æ·»åŠ å›¾åƒ
            if images:
                for img_tensor in images:
                    try:
                        # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸€å¼ å›¾ç‰‡
                        batch_size = img_tensor.shape[0]
                        for i in range(batch_size):
                            single_image = img_tensor[i]
                            base64_image = encode_image_tensor(single_image)
                            user_content.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}"
                                }
                            })
                    except Exception as e:
                        _log_error(f"å¤„ç†å›¾åƒå¤±è´¥: {e}")
            
            # æ„é€  messages
            if not images and len(user_content) == 1 and user_content[0]["type"] == "text":
                 messages.append({"role": "user", "content": user_message})
            else:
                 messages.append({"role": "user", "content": user_content})
            
            payload = {
                "model": model_name,
                "messages": messages,
                "max_tokens": max_tokens,
                "stream": False
            }
            
            # é’ˆå¯¹ reasoning (æ¨ç†) æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
            if not is_reasoning_model:
                payload["temperature"] = temperature
                payload["top_p"] = top_p
                # OpenAI ç§å­å‚æ•°
                if effective_seed != 0:
                    payload["seed"] = effective_seed
            else:
                _log_info(f"æ£€æµ‹åˆ°æ¨ç†æ¨¡å‹ ({model_name})ï¼Œå·²è‡ªåŠ¨ç§»é™¤ temperature, top_p, max_tokens å’Œ seed å‚æ•°ä»¥é¿å… 422/400 é”™è¯¯")
                # æ¨ç†æ¨¡å‹é€šå¸¸ä¸æ¥å— max_tokens (æ”¹ç”¨ max_completion_tokens) æˆ– seed
                if "max_tokens" in payload:
                    # OpenAI o1 ä½¿ç”¨ max_completion_tokensï¼Œè¿™é‡Œå…ˆç§»é™¤ max_tokens
                    # å¦‚æœéœ€è¦æ”¯æŒ max_completion_tokensï¼Œå¯ä»¥æ·»åŠ 
                    del payload["max_tokens"]
                    # payload["max_completion_tokens"] = max_tokens # å¯é€‰

            timeout = self.config.get("timeout", 120)
            
            # æ‰“å°æœ€ç»ˆ payload ç”¨äºè°ƒè¯•
            _log_info(f"Request Payload Keys: {list(payload.keys())}")
            
            # å‘é€è¯·æ±‚
            response = requests.post(url, headers=headers, json=payload, timeout=timeout, verify=False)
            
            if response.status_code != 200:
                error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                _log_error(error_msg)
                return ("", str(response.text), f"âŒ API è°ƒç”¨å¤±è´¥ï¼š{error_msg}")
            
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                response_text = result["choices"][0]["message"]["content"]
                _log_info(f"APIè°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆé•¿åº¦: {len(response_text)} å­—ç¬¦")
            else:
                error_msg = f"å“åº”æ ¼å¼é”™è¯¯: {result}"
                _log_error(error_msg)
                return ("", str(result), f"âŒ å“åº”æ ¼å¼é”™è¯¯ï¼š{error_msg}")
            
            # === ç”Ÿæˆè¯¦ç»†ä¿¡æ¯ ===
            info_lines = [
                "=" * 50,
                "ğŸ‰ GPT å¯¹è¯æˆåŠŸ",
                "=" * 50,
                "",
                "ğŸ“Š å¯¹è¯ç»Ÿè®¡ï¼š",
                *[f"   {info}" for info in status_info],
                f"   ğŸ“ å›å¤é•¿åº¦ï¼š{len(response_text)} å­—ç¬¦",
                f"   ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯é•¿åº¦ï¼š{len(user_message)} å­—ç¬¦",
                "",
                "ğŸ¤– API å‚æ•°ï¼š",
                f"   ğŸŒ¡ï¸ æ¸©åº¦ï¼š{temperature}",
                f"   ğŸ¯ Top-Pï¼š{top_p}",
                f"   ğŸ“ æœ€å¤§é•¿åº¦ï¼š{max_tokens}",
                "",
                "=" * 50
            ]
            
            info = "\n".join(info_lines)
            
            _log_info("âœ… GPT å¯¹è¯å®Œæˆï¼")
            return (response_text, response_text, info)
            
        except Exception as e:
            # å°è¯•è·å–æ›´è¯¦ç»†çš„å“åº”ä¿¡æ¯
            error_details = str(e)
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_json = e.response.json()
                    error_details = f"{e.response.status_code} - {json.dumps(error_json, ensure_ascii=False)}"
                except:
                    error_details = f"{e.response.status_code} - {e.response.text}"
            
            _log_error(f"APIè°ƒç”¨å¤±è´¥: {error_details}")
            
            return ("", f"Error: {error_details}", f"âŒ APIè°ƒç”¨å¤±è´¥: {error_details}")


# ==================== èŠ‚ç‚¹æ³¨å†Œ ====================

NODE_CLASS_MAPPINGS = {
    "GPT_Multimodal_Chat": GPT_Multimodal_Chat,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GPT_Multimodal_Chat": "ğŸ¤– GPT å¤šæ¨¡æ€å¯¹è¯ (OpenAI/T8) @ç‚®è€å¸ˆçš„å°è¯¾å ‚",
}
